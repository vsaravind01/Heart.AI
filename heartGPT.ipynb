{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CREATION AND UPLOADING EMBEDDINGS IN VECTOR DB"
      ],
      "metadata": {
        "id": "DACrb_yqvqu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This section takes a cardiology-related PDF, breaks its pages into smaller chunks, encodes these chunks into numerical representations (embeddings) using Sentence Transformers, and then uploads these embeddings to a QDrant cloud instance for storage. This process allows for efficient storage and retrieval of semantic information from the document in the cloud-based database."
      ],
      "metadata": {
        "id": "oCXxQWRC0L5N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vF1c8pQSCbW"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentence_transformers\n",
        "!pip install qdrant-client\n",
        "!pip -q install --upgrade together\n",
        "!pip -q install langchain\n",
        "!pip install fitz\n",
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers.util import cos_sim\n",
        "from jinja2 import Template\n",
        "import pandas as pd\n",
        "from qdrant_client.http import models\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "from qdrant_client import QdrantClient\n",
        "import os\n",
        "import together\n",
        "from langchain.vectorstores import Qdrant\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from IPython.display import display, Markdown, Latex\n",
        "import fitz\n",
        "import grpc"
      ],
      "metadata": {
        "id": "0k83WTUsTEN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TOGETHER_API_KEY\"] = \"148521c4088ad416dced465cc144671626b00c860af4e6ebc855953567087d8a\""
      ],
      "metadata": {
        "id": "GFGrhnSzTF4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qdrant_client = QdrantClient(\n",
        "    \"https://a7f256c7-0d4f-4f06-a2be-7ea2de3e09fc.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
        "    prefer_grpc=True,\n",
        "    api_key=\"7xkDRSDVb-jmBdGOFL5RuRCcV2UHPxCWfT3EbFrApwizz2EgZTClPA\",\n",
        ")"
      ],
      "metadata": {
        "id": "L0cIokHATKLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(text_batch):\n",
        "  embedding_model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\n",
        "  text_embeds = embedding_model.encode(text_batch)\n",
        "  return text_embeds"
      ],
      "metadata": {
        "id": "jJezBXPmTHwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_QDrant_collection():\n",
        "  embedding_model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\n",
        "  qdrant_client.recreate_collection(\n",
        "\tcollection_name=\"cardiology_docs\",\n",
        "\tvectors_config=models.VectorParams(\n",
        "\t\tsize=embedding_model.get_sentence_embedding_dimension(),\n",
        "\t\tdistance=models.Distance.COSINE\n",
        "\t)\n",
        ")\n",
        "\n",
        "create_QDrant_collection()"
      ],
      "metadata": {
        "id": "sJKLD_rZTMH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers.util import cos_sim\n",
        "\n",
        "model1 = SentenceTransformer('thenlper/gte-base')"
      ],
      "metadata": {
        "id": "ZiVhjxNaWCHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.get_sentence_embedding_dimension()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOhyZb5vWJ7O",
        "outputId": "92090de0-ac82-4be0-859a-1291b28c39a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pdf_and_upload_to_qdrant(pdf_path):\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "\n",
        "    chunks = []\n",
        "    for page_num in range(pdf_document.page_count):\n",
        "        page = pdf_document.load_page(page_num)\n",
        "        text = page.get_text(\"text\")\n",
        "        chunk_size = 5000\n",
        "        chunks.extend([text[i:i + chunk_size] for i in range(0, len(text), chunk_size)])\n",
        "\n",
        "    records_to_upload = []\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        vector = model1.encode(chunk).tolist()\n",
        "\n",
        "        record = models.Record(\n",
        "            id=idx,\n",
        "            vector=vector,\n",
        "            payload={\"page_content\": chunk}\n",
        "        )\n",
        "        records_to_upload.append(record)\n",
        "\n",
        "    qdrant_client.upload_records(\n",
        "        collection_name=\"cardiology_docs\",\n",
        "        records=records_to_upload\n",
        "    )\n",
        "\n",
        "    pdf_document.close()\n"
      ],
      "metadata": {
        "id": "3MnTCFqhdh22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_pdf():\n",
        "  try:\n",
        "      process_pdf_and_upload_to_qdrant(\"/content/cardiac_surgery.pdf\")\n",
        "  except grpc.RpcError as e:\n",
        "      print(f\"Error communicating with Qdrant: {e}\")\n",
        "\n",
        "upload_pdf()"
      ],
      "metadata": {
        "id": "TDGBCC9Bh0PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hits = qdrant_client.search(\n",
        "\tcollection_name=\"cardiology_docs\",\n",
        "\tquery_vector=model1.encode(\"SPPB\").tolist(),\n",
        "\tlimit=3\n",
        ")\n",
        "for hit in hits:\n",
        "\tprint(hit.payload, \"score:\", hit.score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWCmWFCriItT",
        "outputId": "6853f798-afa8-496c-8334-46daf737feda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page_content': 'f 1 foot in front of and touching\\nthe toes of the other foot for about 10 s)\\n2. Gait speed (5-meters, see above), assessed 2 times\\n3. Chair stand test (getting up from a chair with arms folded in\\nfront of the chest 5 times)\\nConsensus statements: The Short Physical Performance Battery as\\na prediction tool for mortality after cardiac surgery\\nUsage of the SPPB is advised to assess frailty in patients undergoing car-\\ndiac surgery to predict intermediate-term mortality [45, 52].\\nSPPB: Short Physical Performance Battery.\\nConsensus statements: Composite indexes as prediction tools for\\nmortality after cardiac surgery\\nThe CAF test is advised to assess frailty in patients undergoing cardiac\\nsurgery to predict short, intermediate- and long-term mortality [16, 21,\\n54–56].\\nThe EFT may be used to assess frailty in patients undergoing cardiac sur-\\ngery to predict short and intermediate term mortality [48, 62].\\nCAF: comprehensive assessment of frailty; EFT: Essential Frailty Toolset\\n10\\nS.H. Su¨ndermann et al. / European Journal of Cardio-Thoracic Surgery\\nDownloaded from https://academic.oup.com/ejcts/article/64/4/ezad181/7296022 by guest on 09 December 2023\\n'} score: 0.7974008917808533\n",
            "{'page_content': 'ion; MoCA: Montreal Cognitive Assessment; PAI:\\npsoas muscle area index; SPPB: Short Physical Performance Battery.\\nConsensus statements: prediction of quality of life, discharge loca-\\ntion and probability of readmission after cardiac surgery\\nAssessment of Fried criteria is advised to estimate the probability of re-\\nadmission and discharge to an intermediate-care facility [6, 30, 74, 75–\\n78].\\nIf part of routine CT scans, assessment of psoas muscle size may be used\\nto estimate the probability of discharge to an intermediate-care facility\\nor a location other than home [29, 30, 37, 75–78].\\nFried criteria assessment may be used to estimate QoL after cardiac sur-\\ngery [30, 75–78, 97].\\nCT: computed tomography; QoL: quality of life.\\nCONSENSUS STATEMENT\\n15\\nS.H. Su¨ndermann et al. / European Journal of Cardio-Thoracic Surgery\\nDownloaded from https://academic.oup.com/ejcts/article/64/4/ezad181/7296022 by guest on 09 December 2023\\n'} score: 0.7757360339164734\n",
            "{'page_content': 'ownloaded from https://academic.oup.com/ejcts/article/64/4/ezad181/7296022 by guest on 09 December 2023\\n'} score: 0.7737544775009155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHAT COMPLETION USING TOGETHER"
      ],
      "metadata": {
        "id": "ezIQuzD3wnkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leveraging the Together.ai, this section uses the llama-2-70b-chat llm for chat completion. Moreover with the help of QDrant DB, the relavent docs are fetched and provided into the model as its internal knowledge."
      ],
      "metadata": {
        "id": "zrAdXcWf0a8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "\n",
        "import logging\n",
        "from typing import Any, Dict, List, Mapping, Optional\n",
        "\n",
        "from pydantic import Extra, Field, field_validator\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.llms.utils import enforce_stop_tokens\n",
        "from langchain.utils import get_from_dict_or_env\n",
        "\n",
        "class TogetherLLM(LLM):\n",
        "    \"\"\"Together large language models.\"\"\"\n",
        "\n",
        "    model: str = \"togethercomputer/llama-2-70b-chat\"\n",
        "    \"\"\"model endpoint to use\"\"\"\n",
        "\n",
        "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
        "    \"\"\"Together API key\"\"\"\n",
        "\n",
        "    temperature: float = 0.7\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    max_tokens: int = 512\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    class Config:\n",
        "        extra = 'forbid'\n",
        "\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the API key is set.\"\"\"\n",
        "        api_key = get_from_dict_or_env(\n",
        "            values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
        "        )\n",
        "        values[\"together_api_key\"] = api_key\n",
        "        return values\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"together\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Call to Together endpoint.\"\"\"\n",
        "        together.api_key = self.together_api_key\n",
        "        output = together.Complete.create(prompt,\n",
        "                                          model=self.model,\n",
        "                                          max_tokens=self.max_tokens,\n",
        "                                          temperature=self.temperature,\n",
        "                                          )\n",
        "        text = output['output']['choices'][0]['text']\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "FtkQ1toETowX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "together.api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
        "together.Models.start(\"togethercomputer/llama-2-7b-chat\")\n",
        "\n",
        "llm = TogetherLLM(\n",
        "    model= \"togethercomputer/llama-2-7b-chat\",\n",
        "    temperature = 0.1,\n",
        "    max_tokens = 1024\n",
        ")\n",
        "\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n",
        "\n",
        "qdrant = Qdrant(\n",
        "    client=qdrant_client,\n",
        "    collection_name=\"cardiology_docs\",\n",
        "    embeddings=embeddings,\n",
        ")\n",
        "\n",
        "retriever = qdrant.as_retriever()\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer in English:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, chain_type_kwargs=chain_type_kwargs)"
      ],
      "metadata": {
        "id": "cuADtVZpoW2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HeartGPT"
      ],
      "metadata": {
        "id": "Z6C3zDPExZsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HeartGPT, a chatbot, which takes any cardiology-related user query as input, retrieves the relavent docs from the vector db which is fed into the chat completion model to answer the users query."
      ],
      "metadata": {
        "id": "Evkym95O1h3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_result(result):\n",
        "  output_text = f\"\"\"### Question:\n",
        "  {query}\n",
        "  ### Answer:\n",
        "  {result}\"\"\"\n",
        "  return (output_text)"
      ],
      "metadata": {
        "id": "7K3ps6oFqz1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Tell me about Short Physical Performance Battery\"\n",
        "result = qa.run(query)\n",
        "display(Markdown(print_result(result)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "dpMQIQPSq4SP",
        "outputId": "203987d4-87fa-45e3-fa05-0889e2902dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Question:\n  Tell me about Short Physical Performance Battery\n  ### Answer:\n  \nThe Short Physical Performance Battery (SPPB) is a predictive tool for mortality after cardiac surgery. It comprises 3 batteries of tests: balance (standing with feet side by side, semi-tandem, and tandem), gait speed, and the chair stand test. The SPPB has been shown to add predictive value for the estimation of perioperative and intermediate-term mortality. It is an easily accessible functional test with 3 components, and it has been used in 4 studies to predict 1-year mortality. The SPPB is a standardized subjective estimation of a patient's frailty status, but it is not an eyeball test and requires training."
          },
          "metadata": {}
        }
      ]
    }
  ]
}